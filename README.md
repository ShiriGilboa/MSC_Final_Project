# STT Post-Processing Evaluation Project

A comprehensive evaluation system for Speech-to-Text (STT) post-processing using Large Language Models (LLMs). This project loads NBA audio transcripts from Hugging Face and runs 4 specialized pipelines to enhance and analyze the content.

## ğŸ€ Dataset

- **Source**: [ShiriGilboa/my-nba-dataset](https://huggingface.co/datasets/ShiriGilboa/my-nba-dataset)
- **Content**: 421 NBA audio samples with transcriptions
- **Features**: Audio files + text transcriptions

## ğŸ”§ Pipelines

1. **GenerateWhisperPromptPipeline** - Generates context-aware prompts for Whisper STT
2. **FixTranscriptByLLMPipeline** - Post-processes and corrects STT transcripts  
3. **GenerateNamesPipeline** - Extracts named entities from transcripts
4. **GenerateTopicPipeline** - Identifies conversation topics

## ğŸš€ Quick Start

### Prerequisites

```bash
# Install dependencies
pip install -r requirements.txt

# Set your OpenAI API key
export OPENAI_API_KEY="your-api-key-here"
```

### Running the Evaluation

```bash
# Run with default settings (10 samples)
python main.py --api-key YOUR_OPENAI_API_KEY

# Run with more samples
python main.py --api-key YOUR_OPENAI_API_KEY --samples 50

# Run with verbose output
python main.py --api-key YOUR_OPENAI_API_KEY --samples 20 --verbose
```

### Using Conda Environment

```bash
# Activate conda environment (if available)
conda activate whisper-llm

# Run the evaluation
python main.py --api-key YOUR_OPENAI_API_KEY
```

## ğŸ“Š Output

The script generates:
- **Console output**: Real-time progress and results
- **CSV file**: `data/pipeline_evaluation_results.csv` with detailed results
- **Summary statistics**: Success rates and performance metrics

## ğŸ›ï¸ Dashboard

Launch the interactive dashboard to visualize results:

```bash
streamlit run src/dashboard/main.py
```

## ğŸ“ Project Structure

```
stt_evaluation_project/
â”œâ”€â”€ main.py                 # Main entry point
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ config.py              # Configuration
â”œâ”€â”€ data/                  # Results and data
â””â”€â”€ src/
    â”œâ”€â”€ core/
    â”‚   â””â”€â”€ pipelines.py   # Pipeline implementations
    â”œâ”€â”€ agents/
    â”‚   â””â”€â”€ llm_agents.py  # LLM agent classes
    â”œâ”€â”€ utils/             # Utility functions
    â””â”€â”€ dashboard/
        â””â”€â”€ main.py        # Streamlit dashboard
```

## ğŸ” Features

- **Dataset Loading**: Automatic loading from Hugging Face
- **Parallel Processing**: Efficient pipeline execution
- **Error Handling**: Robust error management
- **Results Export**: CSV export for analysis
- **Interactive Dashboard**: Visual result exploration
- **Configurable**: Adjustable sample sizes and verbosity

## ğŸ“ˆ Results Analysis

The generated CSV contains:
- Sample metadata (ID, duration, transcript length)
- Pipeline results (success status, output)
- Performance metrics (processing time, result quality)
- Timestamps and source information

## ğŸ› ï¸ Development

The project is structured for easy extension:
- Add new pipelines by extending `BasePipeline`
- Modify agent instructions in `src/utils/instructions.py`
- Customize evaluation metrics in the main script

## ğŸ“ License

This project is part of an academic evaluation system for STT post-processing research.
